import pandas as pd
import numpy as np
import joblib
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns
import os

sns.set_style("whitegrid")

# --- Step 6: Interpretation & Insights ---

print("\n--- Step 6: Interpretation & Insights ---")

final_features_data_path = 'data/final_features.csv'

try:
    df_final = pd.read_csv(final_features_data_path)
    print(f"Final features dataset '{final_features_data_path}' loaded for Interpretation.")
except FileNotFoundError:
    print(f"Error: '{final_features_data_path}' not found.")
    print("Please ensure the file is generated by '02_feature_engineering.py' and is in the 'data/' directory.")
    exit()

X = df_final.drop('Churn', axis=1)
y = df_final['Churn']

# Re-split data to get X_train for consistent column ordering
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Load the saved scaler (needed for scaling feature names if interpreting scaled features)
try:
    scaler = joblib.load('models/scaler.pkl')
    print("Scaler loaded.")
except FileNotFoundError:
    print("Error: 'scaler.pkl' not found in 'models/' directory.")
    print("Please ensure '03_model_building.py' was run to save the scaler.")
    exit()

# Load the best performing model for interpretation.
# Typically, Random Forest or Gradient Boosting are good choices for feature importances.
# Let's use Random Forest as an example, but you can choose based on your evaluation results.
try:
    best_model = joblib.load('models/random_forest_model.pkl')
    model_name = "Random Forest"
    print(f"Best model ({model_name}) loaded for interpretation.")
except FileNotFoundError:
    print("Error: Random Forest model not found in 'models/' directory.")
    print("Please ensure '03_model_building.py' was run to save the model.")
    # Fallback to another model if RF not found or if a different model was best
    try:
        best_model = joblib.load('models/gradient_boosting_model.pkl')
        model_name = "Gradient Boosting"
        print(f"Best model ({model_name}) loaded for interpretation.")
    except FileNotFoundError:
        print("Error: Gradient Boosting model also not found. Cannot proceed with interpretation.")
        exit()


# 1. Feature Importance (for tree-based models like Random Forest, Gradient Boosting)
if hasattr(best_model, 'feature_importances_'):
    importances = best_model.feature_importances_
    features = X_train.columns
    feature_importance_df = pd.DataFrame({'Feature': features, 'Importance': importances})
    feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

    print(f"\nTop 15 Feature Importances from {model_name}:")
    print(feature_importance_df.head(15))

    plt.figure(figsize=(12, 8))
    sns.barplot(x='Importance', y='Feature', data=feature_importance_df.head(15), palette='viridis')
    plt.title(f'Top 15 Feature Importances ({model_name})')
    plt.xlabel('Importance')
    plt.ylabel('Feature')
    plt.show()

elif hasattr(best_model, 'coef_'):
    # For linear models like Logistic Regression
    print("\nModel is a linear model. Interpreting coefficients.")
    # Coefficients are for scaled features, so match with scaled feature names
    coefficients = best_model.coef_[0]
    features = X_train.columns
    coeff_df = pd.DataFrame({'Feature': features, 'Coefficient': coefficients})
    coeff_df = coeff_df.sort_values(by='Coefficient', ascending=False)

    print("\nTop 15 Positive Coefficients (driving churn):")
    print(coeff_df.head(15))
    print("\nTop 15 Negative Coefficients (preventing churn):")
    print(coeff_df.tail(15))

    plt.figure(figsize=(12, 8))
    sns.barplot(x='Coefficient', y='Feature', data=coeff_df.head(15), palette='viridis')
    plt.title(f'Top 15 Positive Coefficients ({model_name})')
    plt.xlabel('Coefficient Value')
    plt.ylabel('Feature')
    plt.show()

    plt.figure(figsize=(12, 8))
    sns.barplot(x='Coefficient', y='Feature', data=coeff_df.tail(15), palette='viridis')
    plt.title(f'Top 15 Negative Coefficients ({model_name})')
    plt.xlabel('Coefficient Value')
    plt.ylabel('Feature')
    plt.show()
else:
    print("\nSelected model does not have direct feature importance or coefficient attributes for easy interpretation.")


# 2. Analyze the Impact of Key Features on Churn (Visualizations)
print("\n--- Analyzing Impact of Key Features on Churn ---")

# Example: Impact of 'Contract' type
# If 'Contract' was one-hot encoded, you'll have columns like 'Contract_One year', 'Contract_Two year'
# Let's use the original 'Contract' column from df_final if available, otherwise use encoded.
if 'Contract' in df_final.columns:
    plt.figure(figsize=(8, 6))
    sns.countplot(x='Contract', hue='Churn', data=df_final, palette='coolwarm')
    plt.title('Churn by Contract Type')
    plt.xlabel('Contract Type')
    plt.ylabel('Number of Customers')
    plt.show()
else:
    print("\nOriginal 'Contract' column not found for direct visualization. Assuming it's one-hot encoded.")
    # If one-hot encoded, visualize the engineered features from it
    contract_cols = [col for col in df_final.columns if 'Contract_' in col]
    if contract_cols:
        contract_churn_rates = []
        for col in contract_cols:
            churn_rate = df_final[df_final[col] == 1]['Churn'].mean() * 100
            contract_churn_rates.append({'Contract Type': col.replace('Contract_', ''), 'Churn Rate': churn_rate})
        contract_churn_df = pd.DataFrame(contract_churn_rates)
        plt.figure(figsize=(8, 6))
        sns.barplot(x='Contract Type', y='Churn Rate', data=contract_churn_df, palette='viridis')
        plt.title('Churn Rate by Contract Type (Encoded)')
        plt.xlabel('Contract Type')
        plt.ylabel('Churn Rate (%)')
        plt.show()
    else:
        print("No 'Contract' related columns found for analysis.")


# Example: Impact of 'InternetService'
if 'InternetService' in df_final.columns:
    plt.figure(figsize=(8, 6))
    sns.countplot(x='InternetService', hue='Churn', data=df_final, palette='coolwarm')
    plt.title('Churn by Internet Service Type')
    plt.xlabel('Internet Service Type')
    plt.ylabel('Number of Customers')
    plt.show()
else:
    print("\nOriginal 'InternetService' column not found for direct visualization. Assuming it's one-hot encoded.")
    internet_cols = [col for col in df_final.columns if 'InternetService_' in col]
    if internet_cols:
        internet_churn_rates = []
        for col in internet_cols:
            churn_rate = df_final[df_final[col] == 1]['Churn'].mean() * 100
            internet_churn_rates.append({'Internet Service': col.replace('InternetService_', ''), 'Churn Rate': churn_rate})
        internet_churn_df = pd.DataFrame(internet_churn_rates)
        plt.figure(figsize=(8, 6))
        sns.barplot(x='Internet Service', y='Churn Rate', data=internet_churn_df, palette='viridis')
        plt.title('Churn Rate by Internet Service Type (Encoded)')
        plt.xlabel('Internet Service Type')
        plt.ylabel('Churn Rate (%)')
        plt.show()
    else:
        print("No 'InternetService' related columns found for analysis.")


# Example: Impact of 'MonthlyCharges' and 'Tenure'
plt.figure(figsize=(10, 7))
sns.scatterplot(x='MonthlyCharges', y='TotalCharges', hue='Churn', size='tenure', sizes=(20, 400),
                data=df_final, palette='coolwarm', alpha=0.6)
plt.title('Churn by Monthly vs. Total Charges, colored by Tenure')
plt.xlabel('Monthly Charges')
plt.ylabel('Total Charges')
plt.show()

# Additional interpretation based on numerical features like tenure
plt.figure(figsize=(8, 6))
sns.violinplot(x='Churn', y='tenure', data=df_final, palette='pastel')
plt.title('Tenure Distribution by Churn')
plt.xlabel('Churn (0=No, 1=Yes)')
plt.ylabel('Tenure (months)')
plt.show()

plt.figure(figsize=(8, 6))
sns.violinplot(x='Churn', y='MonthlyCharges', data=df_final, palette='pastel')
plt.title('Monthly Charges Distribution by Churn')
plt.xlabel('Churn (0=No, 1=Yes)')
plt.ylabel('Monthly Charges')
plt.show()

print("\nInterpretation insights generated. Review plots and printed feature importances.")